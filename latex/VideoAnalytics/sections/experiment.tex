\section{Experiments}
\label{sec:experiments}

\subsection{Setup}
We implement a custom Python simulation to emulate a crowd-powered video analytics platform. All experiments run on a local desktop (Intel Core i5-13400F, 10 cores/16 threads, 2.50--4.0\,GHz) without GPU acceleration. The simulator generates synthetic task streams, models heterogeneous mobile workers, and executes the full scheduling loop including context normalization, quality estimation, and assignment.

We compare four policies:
\textbf{Ours} (our method, Hungarian matching with adaptive partitioning),
\textbf{Greedy} (ranking candidates by estimated reward and greedily selecting non-conflicting pairs),
\textbf{Random} (baseline without learning), and an \textbf{Oracle} that knows the ground-truth success probability $p(x)$ and serves as an upper bound.
All methods share the same pre-generated task streams and reward samples (Bernoulli draws with a fixed seed) to ensure a fair comparison.

Unless otherwise stated, we enable worker dynamics so that the worker pool evolves during training. This highlights the ability of Ours to adapt to changing supply while maintaining a stable advantage over Greedy/Random.

\subsection{Workload and Environment}
The simulated platform contains a task queue, a dynamic worker pool (initially 10 workers), and a context normalizer. Each round consists of new task arrivals, candidate generation, matching, execution, and online model updates.

\textbf{Task arrival:} At step $t$ we sample $N_t \sim \mathcal{U}\{6, \ldots, 15\}$ tasks. Each task carries attributes such as type, data size, deadline, and implicit quality preferences. We run $T=1000$ time steps in the main experiments.

\textbf{Worker features:} Workers are characterised by driving speed, bandwidth, processor speed, physical distance, task type compatibility, data size tolerance, and weather robustness. All features are normalized into $[0,1]$ using the ranges in \texttt{WORKER\_FEATURE\_VALUES\_RANGE}.

\textbf{Worker dynamics:} At every step each worker independently leaves with probability $0.03$. New workers join with probability $0.15$ and a batch size sampled from $\{1,2,3\}$. This keeps the expected number of active workers close to ten, mimicking a moderately dynamic deployment.

\subsection{Methods}

\textbf{Ours (proposed):} We maintain an adaptive partition tree over the context space. Each leaf stores a Beta posterior with prior $\alpha_0,\beta_0$ and observed successes/failures. When the posterior variance remains high after observing \texttt{PARTITION\_MIN\_SAMPLES} assignments, the leaf is split along the longest dimension. At decision time, we compute the posterior mean (optionally plus UCB bonus) minus replication cost to form a net reward matrix and solve a one-to-one matching with the Hungarian algorithm. Only positive-net pairs are retained.

\textbf{Greedy (baseline):} Uses the same posterior estimates as Ours but ranks candidate pairs in descending order of estimated net reward and greedily picks non-conflicting pairs until no positive-net option remains. This baseline reflects the widely-used ``max score per assignment'' heuristic.

\textbf{Random (baseline):} Filters out non-positive pairs, randomly shuffles the rest, and greedily selects non-conflicting pairs.

\textbf{Oracle (upper bound):} Uses the ground-truth $p(x)$ to fill the net reward matrix ($p(x)-c$) and runs the Hungarian algorithm. Oracle does not learn; it only serves as an upper bound in the plots.

\subsection{Success Probability and Reward}
Given a normalized context vector $x \in [0,1]^d$, the success probability $p(x)$ is simulated using a smooth, nonlinear function that includes:

- Diminishing marginal returns (via square root) and pairwise interactions (via geometric mean) for \textbf{positive features} such as driving speed, bandwidth, and processor performance;
- Convex penalties for \textbf{negative features} such as distance, data size, and weather;
- A soft gating effect modeled by a sigmoid function to simulate bottleneck behavior.

The execution reward is drawn from a Bernoulli distribution $\text{Bernoulli}(p(x))$ per assignment. Each assignment incurs a unit replication cost of $c=0.1$, and the net reward is defined as reward minus cost.

\subsection{Metrics}

\textbf{Loss (regret) per step:} Taking the oracle’s expected net reward as the upper bound, the step loss is defined as
\[
\text{loss}_t = \max\left(0, \ \mathbb{E}[\text{net}]_{\text{oracle}} - \mathbb{E}[\text{net}]_{\text{alg}}\right).
\]

\textbf{Cumulative Net Reward:} The total net reward is computed as
\[
\sum_{t} (\text{reward}_t - c),
\]
where $\text{reward}_t$ is the sampled binary success feedback drawn from $p(x)$.

To ensure fairness, all algorithms run independently on the same task stream, and their performance curves are plotted accordingly.

\subsection{Hyperparameters}

The default hyperparameters used in the experiments are:

\begin{itemize}
    \item Replication cost: $c = 0.2$ (\texttt{REPLICATION\_COST})
    \item Maximum replicas per task: $B = 1$ (\texttt{budget})
    \item Partition split threshold: $\tau = 10$ (\texttt{partition\_split\_threshold})
    \item Minimum samples before split: $6$ (\texttt{PARTITION\_MIN\_SAMPLES})
    \item Variance threshold for split: $0.01$ (\texttt{PARTITION\_VARIANCE\_THRESHOLD})
    \item Maximum partition depth: $D = 64$ (\texttt{MAX\_PARTITION\_DEPTH})
    \item Split strategy: longest dimension (\texttt{PARTITION\_SPLIT\_STRATEGY=longest})
    \item Batch size: 10 (\texttt{COMPARISON\_BATCH\_SIZE})
    \item Total steps: 1000 (\texttt{COMPARISON\_STEPS})
    \item Random seed: 43 (\texttt{RANDOM\_SEED})
\end{itemize}

\subsection{Results}

Figure~\ref{fig:loss} plots the per-step loss (regret). Ours quickly drives the regret below both Greedy and Random, confirming that adaptive partitioning accelerates convergence even under worker arrivals/departures. For a clearer long-term trend we also report a smoothed view (Figure~\ref{fig:loss_smooth}) obtained via a rolling mean (window $=50$) together with a 10--90\% quantile band; the smoothed curve shows Ours consistently maintaining the lowest regret plateau.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{VideoAnalytics/fig/compare_loss.png}
\caption{Per-step loss. Ours vs. Greedy vs. Random.}
\label{fig:loss}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{VideoAnalytics/fig/compare_loss_smooth.png}
\caption{Smoothed per-step loss (rolling mean window $=50$, shaded area: 10--90\% percentile band).}
\label{fig:loss_smooth}
\end{figure}

Figure~\ref{fig:cum} further shows that Ours accumulates the highest net reward realized and tracks the Oracle upper bound much more closely than the baselines.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{VideoAnalytics/fig/compare_cum_reward.png}
\caption{Realized cumulative net reward. Ours remains ahead of Greedy/Random while approaching Oracle.}
\label{fig:cum}
\end{figure}

\paragraph{Challenge scenario.}\label{sec:challenge}
We further construct a conflict-heavy benchmark where different task clusters strongly favor different worker specialties. In this regime, Greedy often allocates the same “universal” workers to multiple clusters and leaves other tasks under-served, while Ours uses the Hungarian solver to coordinate assignments globally. Figure~\ref{fig:challenge_totals} shows that Ours accumulates a larger reward margin over Greedy in this setting.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{VideoAnalytics/fig/challenge_totals.png}
\caption{Conflict-heavy benchmark (Section~\ref{sec:challenge}): Ours secures a larger margin over Greedy.}
\label{fig:challenge_totals}
\end{figure}

\subsection{Ablations and Reproducibility}

\textbf{Worker Dynamics.} All reported experiments enable the dynamic-arrival setting (leave probability 0.03, join probability 0.15 with batches of 1–3). For ablations we additionally run a static variant by disabling dynamics.

\textbf{Task Stream and Randomness.} The task stream is pre-generated and shared across all methods. Randomness in reward sampling is controlled using a fixed global seed.

\textbf{Reproducibility.} The experiments can be reproduced by running the main script \texttt{scheduler.py} with \texttt{RUN\_COMPARISON=True}. Parameters such as step count, batch size, and arrival range can be adjusted in \texttt{config.py}.
